{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f9ce70-f5b9-44f7-a8a1-506b8a7c92c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T10:08:58.985296Z",
     "iopub.status.busy": "2023-04-15T10:08:58.984964Z",
     "iopub.status.idle": "2023-04-15T10:08:58.988709Z",
     "shell.execute_reply": "2023-04-15T10:08:58.987984Z",
     "shell.execute_reply.started": "2023-04-15T10:08:58.985253Z"
    },
    "tags": []
   },
   "source": [
    "# 노트북에서 모델 학습 및 서빙 API 생성 파이프라인 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1da6808-809e-4808-ace3-007b3af063fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'json_loads' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_version\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from kakaocloud_kbm import KbmPipelineClient\n",
    "import kfp\n",
    "from kfp import kubernetes\n",
    "from kfp import components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import Output, Input, Artifact, Model, Dataset, InputPath, OutputPath\n",
    "import kfp.compiler as compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1e8ef6-ca73-45b2-93ee-a0fd3e6a01ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kakaocloud_kbm/__init__.py:184: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# KBM Kubeflow SDK\n",
    "os.environ[\"KUBEFLOW_HOST\"] = \"https://{{ KUBEFLOW 도메인 주소}}\" # 반드시 Kubeflow 생성시 입력하신 IP형태가 아닌 도메인을 넣어주셔야 합니다.\n",
    "os.environ[\"KUBEFLOW_USERNAME\"] = \"{{ KUBEFLOW 계정 이메일 }}\"\n",
    "os.environ[\"KUBEFLOW_PASSWORD\"] = \"{{ KUBEFLOW 계정 비밀번호 }}\"\n",
    "\n",
    "client = KbmPipelineClient(\n",
    "    # verify_ssl=False ## 도메인 연결 및 TLS 설정이 되어 있지 않을 경우 활성화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639f3f7a-d56a-4c7f-be57-b83be678a619",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name : torch-model-cdad658c\n",
      "Model PVC Name : test-nlp-pvc-cdad658c\n",
      "Model Server Name : torchserve-cdad658c\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "KBM_NAMESPACE = os.environ['NB_PREFIX'].split('/')[2]\n",
    "COMPONENT_PATH = 'components'\n",
    "SERVE_ENPOINT_PATH = os.path.join(COMPONENT_PATH, 'yelp_review_nlp_serve_model')\n",
    "TRAIN_CR_IMAGE = \"bigdata-150.kr-central-2.kcr.dev/kc-kubeflow/kmlp-pytorch:v1.8.0.py38.cuda.1a\"\n",
    "\n",
    "TASK_UUID = uuid.uuid1().hex[:8]\n",
    "PVC_NAME = f\"test-nlp-pvc-{TASK_UUID}\"\n",
    "MODEL_NAME = f\"torch-model-{TASK_UUID}\"\n",
    "KBM_MODEL_SERV_NAME = f\"torchserve-{TASK_UUID}\"\n",
    "EPOCH_NUM = 10\n",
    "\n",
    "print(f\"Model Name : {MODEL_NAME}\")\n",
    "print(f\"Model PVC Name : {PVC_NAME}\")\n",
    "print(f\"Model Server Name : {KBM_MODEL_SERV_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4fc1be-4c50-4e87-93af-a5e35a71ce73",
   "metadata": {},
   "source": [
    "## 파이프라인 컴포넌트 빌드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0d99b7-b8c8-4069-8c65-51b0a798ce27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "components/yelp_review_nlp_serve_model\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"{SERVE_ENPOINT_PATH}\"\n",
    "\n",
    "mkdir -p ${1}\n",
    "echo ${1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc77d5b-8570-4b7d-9682-39e07dcc7f9d",
   "metadata": {},
   "source": [
    "### 데이터 수집 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0e156c-16c3-4618-a274-bd4e9314ba56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.8' to 'python:3.9' on Oct 1, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.9.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(packages_to_install=['requests'])\n",
    "def download_dataset(\n",
    "    kc_kbm_os_train_url: str,\n",
    "    kc_kbm_os_valid_url: str,\n",
    "    kc_kbm_os_handler_url: str,\n",
    "    kc_kbm_os_kserve_url: str\n",
    "):\n",
    "    import os\n",
    "    from requests import get\n",
    "\n",
    "    def download(url, dist_dir, file_name = None):\n",
    "    \tif not file_name:\n",
    "    \t\tfile_name = url.split('/')[-1]\n",
    "    \n",
    "    \twith open(os.path.join(dist_dir, file_name), \"wb\") as file:   \n",
    "            \tresponse = get(url)               \n",
    "            \tfile.write(response.content)   \n",
    "    \n",
    "    pvc_data_path = \"/data\"\n",
    "\n",
    "    if not kc_kbm_os_train_url:\n",
    "        kc_kbm_os_train_url = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/data/train.csv'\n",
    "        \n",
    "    download(kc_kbm_os_train_url, pvc_data_path, \"train.csv\")\n",
    "\n",
    "    if not kc_kbm_os_valid_url:\n",
    "        kc_kbm_os_valid_url = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/data/validation.csv'\n",
    "\n",
    "    download(kc_kbm_os_valid_url, pvc_data_path, \"validation.csv\")\n",
    "\n",
    "    if not kc_kbm_os_handler_url:\n",
    "        kc_kbm_os_handler_url = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/handler.py'\n",
    "\n",
    "    download(kc_kbm_os_handler_url, pvc_data_path, \"handler.py\")\n",
    "\n",
    "    if not kc_kbm_os_kserve_url:\n",
    "        kc_kbm_os_kserve_url = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/kserve_component.yaml'\n",
    "        \n",
    "    download(kc_kbm_os_kserve_url, pvc_data_path, \"kserve_component.yaml\")\n",
    "\n",
    "    print(os.listdir(pvc_data_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2042056-eeb1-49ef-a176-53fae491f973",
   "metadata": {},
   "source": [
    "### 자연어 처리 모델 학습 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aae22e3-9e21-487a-a1d7-b7a012b5446a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1743/2734480915.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @dsl.component(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'transformers==4.5.1',\n",
    "        'packaging==21.3',\n",
    "        # 'transformers[torch]',\n",
    "        'datasets==1.4.1',\n",
    "        # 'datasets'\n",
    "    ],\n",
    "    base_image=TRAIN_CR_IMAGE,\n",
    "    output_component_file=f'{SERVE_ENPOINT_PATH}/train_component.yaml'\n",
    ")\n",
    "def train_nlp(\n",
    "    epoch_num: str,\n",
    "    model_name: str\n",
    "):\n",
    "    from transformers import (\n",
    "        DistilBertForSequenceClassification,\n",
    "        DistilBertTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        TrainerCallback,\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    pvc_data_path = \"/data\"\n",
    "\n",
    "    print(\"listdir : \\n \", os.listdir())\n",
    "    print(\"getcwd : \\n \", os.getcwd())\n",
    "    \n",
    "    os.chdir(\"/\")\n",
    "        \n",
    "    train_dataset = Dataset.from_pandas(pd.read_csv(f\"{pvc_data_path}/train.csv\")).select(range(32))\n",
    "    evaluation_dataset = Dataset.from_pandas(pd.read_csv(f\"{pvc_data_path}/validation.csv\"))\n",
    "            \n",
    "    print(\"Saving config.properties !!\")\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    def tokenize_function(item):\n",
    "        return tokenizer(\n",
    "            item[\"text\"], \n",
    "            padding=\"max_length\", \n",
    "            max_length=128, \n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    train = train_dataset.map(tokenize_function)\n",
    "    evaluation = evaluation_dataset.map(tokenize_function)\n",
    "    \n",
    "    print(\"complete Tokenizing\")\n",
    "    \n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=len(set(train_dataset[\"label\"]))\n",
    "    )\n",
    "    \n",
    "    tra_arg = TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        output_dir=f\"{pvc_data_path}/torch_model\",\n",
    "        num_train_epochs=int(epoch_num),\n",
    "        logging_steps=2,\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        disable_tqdm=True,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "    \n",
    "    class myCallback(TrainerCallback):\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            print(f\"{state.global_step} Steps \")\n",
    "            \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=tra_arg,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=evaluation,\n",
    "        callbacks=[myCallback],\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    # Saving Tokenizer, Model\n",
    "    trainer.save_model(f\"{pvc_data_path}/torch_model/\")\n",
    "    tokenizer.save_pretrained(f\"{pvc_data_path}/torch_model\")\n",
    "\n",
    "    print(\"Saving Model & Tokenizer Complete !!\")\n",
    "    \n",
    "    # config for torchserve\n",
    "    import json\n",
    "    \n",
    "    config = dict(\n",
    "        inference_address=\"http://0.0.0.0:8085\",\n",
    "        management_address=\"http://0.0.0.0:8085\",\n",
    "        metrics_address=\"http://0.0.0.0:8082\",\n",
    "        grpc_inference_port=7070,\n",
    "        grpc_management_port=7071,\n",
    "        enable_envvars_config=\"true\",\n",
    "        install_py_dep_per_model=\"true\",\n",
    "        model_store=\"model-store\",\n",
    "        model_snapshot=json.dumps({\n",
    "            \"name\": \"startup.cfg\",\n",
    "            \"modelCount\": 1,\n",
    "            \"models\": {\n",
    "                f\"{model_name}\": {  # Model Name\n",
    "                    \"1.0\": {\n",
    "                        \"defaultVersion\": \"true\",\n",
    "                        \"marName\": f\"{model_name}.mar\",\n",
    "                        \"minWorkers\": 1,\n",
    "                        \"maxWorkers\": 5,\n",
    "                        \"batchSize\": 1,\n",
    "                        \"maxBatchDelay\": 10,\n",
    "                        \"responseTimeout\": 60,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }),\n",
    "    )\n",
    "    # creating config & config folder\n",
    "    if not os.path.exists(f\"{pvc_data_path}/torch_model/config\"):\n",
    "        os.mkdir(f\"{pvc_data_path}/torch_model/config\")\n",
    "        \n",
    "    with open(f\"{pvc_data_path}/torch_model/config/config.properties\", \"w\") as f:\n",
    "        for i, j in config.items():\n",
    "            f.write(f\"{i}={j}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bbc0f-bead-4b6f-b7ab-f29a3c6ef3be",
   "metadata": {},
   "source": [
    "### 서빙을 위한 MAR파일 생성 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce43082c-e400-45f8-9021-e733d27ad623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.container_component\n",
    "def create_marfile():\n",
    "    return dsl.ContainerSpec(image='python:3.9', command=[\"/bin/sh\"], args=[\n",
    "        \"-c\",\n",
    "        f\"ls -al /data; cd /data/torch_model; ls -al; pwd; pip install torchserve torch-model-archiver torch-workflow-archiver; torch-model-archiver --model-name {MODEL_NAME} --version 1.0 --serialized-file pytorch_model.bin --handler ../handler.py --extra-files config.json,vocab.txt --force; mkdir model-store; mv -f {MODEL_NAME}.mar model-store; sed -i 's/\\model-store\\b/\\/mnt\\/models\\/model-store/g' /data/torch_model/config/config.properties\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c464786-81ad-48e9-ac1d-d2b22f49f0d6",
   "metadata": {},
   "source": [
    "### 모델(서빙API) 생성 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb308d40-be20-4788-b500-5bd0b176c1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/yelp_review_nlp_serve_model/kserve_component.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SERVE_ENPOINT_PATH}/kserve_component.yaml\n",
    "name: Serve a model with KServe \n",
    "description: Serve Models using KServe \n",
    "inputs:\n",
    "  - {name: Action,                    type: String, default: 'create',                            description: 'Action to execute on KServe'}\n",
    "  - {name: Model Name,                type: String, default: '',                                  description: 'Name to give to the deployed model'}\n",
    "  - {name: Model URI,                 type: String, default: '',                                  description: 'Path of the S3 or GCS compatible directory containing the model.'}\n",
    "  - {name: Canary Traffic Percent,    type: String, default: '100',                               description: 'The traffic split percentage between the candidate model and the last ready model'}\n",
    "  - {name: Namespace,                 type: String, default: '',                                  description: 'Kubernetes namespace where the KServe service is deployed.'}\n",
    "  - {name: Framework,                 type: String, default: '',                                  description: 'Machine Learning Framework for Model Serving.'}\n",
    "  - {name: Runtime Version,           type: String, default: 'latest',                            description: 'Runtime Version of Machine Learning Framework'}\n",
    "  - {name: Resource Requests,         type: String, default: '{\"cpu\": \"0.5\", \"memory\": \"512Mi\"}', description: 'CPU and Memory requests for Model Serving'}\n",
    "  - {name: Resource Limits,           type: String, default: '{\"cpu\": \"1\", \"memory\": \"1Gi\"}',     description: 'CPU and Memory limits for Model Serving'}\n",
    "  - {name: Custom Model Spec,         type: String, default: '{}',                                description: 'Custom model runtime container spec in JSON'}\n",
    "  - {name: Autoscaling Target,        type: String, default: '0',                                 description: 'Autoscaling Target Number'}\n",
    "  - {name: Service Account,           type: String, default: '',                                  description: 'ServiceAccount to use to run the InferenceService pod'}\n",
    "  - {name: Enable Istio Sidecar,      type: Bool,   default: 'True',                              description: 'Whether to enable istio sidecar injection'}\n",
    "  - {name: InferenceService YAML,     type: String, default: '{}',                                description: 'Raw InferenceService serialized YAML for deployment'}\n",
    "  - {name: Watch Timeout,             type: String, default: '300',                               description: \"Timeout seconds for watching until InferenceService becomes ready.\"}\n",
    "  - {name: Min Replicas,              type: String, default: '-1',                                description: 'Minimum number of InferenceService replicas'}\n",
    "  - {name: Max Replicas,              type: String, default: '-1',                                description: 'Maximum number of InferenceService replicas'}\n",
    "  - {name: Request Timeout,           type: String, default: '60',                                description: \"Specifies the number of seconds to wait before timing out a request to the component.\"}\n",
    "  - {name: Enable ISVC Status,        type: Bool,   default: 'True',                              description: \"Specifies whether to store the inference service status as the output parameter\"}\n",
    "\n",
    "outputs:\n",
    "  - {name: InferenceService Status,   type: String,                                               description: 'Status JSON output of InferenceService'}\n",
    "implementation:\n",
    "  container:\n",
    "    image: bigdata-150.kr-central-2.kcr.dev/kc-kubeflow/kserve-component:v0.11.1.kbm.1a\n",
    "    command: ['python']\n",
    "    args: [\n",
    "      -u, kservedeployer.py,\n",
    "      --action,                 {inputValue: Action},\n",
    "      --model-name,             {inputValue: Model Name},\n",
    "      --model-uri,              {inputValue: Model URI},\n",
    "      --canary-traffic-percent, {inputValue: Canary Traffic Percent},\n",
    "      --namespace,              {inputValue: Namespace},\n",
    "      --framework,              {inputValue: Framework},\n",
    "      --runtime-version,        {inputValue: Runtime Version},\n",
    "      --resource-requests,      {inputValue: Resource Requests},\n",
    "      --resource-limits,        {inputValue: Resource Limits},\n",
    "      --custom-model-spec,      {inputValue: Custom Model Spec},\n",
    "      --autoscaling-target,     {inputValue: Autoscaling Target},\n",
    "      --service-account,        {inputValue: Service Account},\n",
    "      --enable-istio-sidecar,   {inputValue: Enable Istio Sidecar},\n",
    "      --output-path,            {outputPath: InferenceService Status},\n",
    "      --inferenceservice-yaml,  {inputValue: InferenceService YAML},\n",
    "      --watch-timeout,          {inputValue: Watch Timeout},\n",
    "      --min-replicas,           {inputValue: Min Replicas},\n",
    "      --max-replicas,           {inputValue: Max Replicas},\n",
    "      --request-timeout,        {inputValue: Request Timeout},\n",
    "      --enable-isvc-status,     {inputValue: Enable ISVC Status}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982dcbfa-1c5a-4265-ae21-422c4c0eb31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.components import load_component_from_file\n",
    "\n",
    "def create_inference_model():\n",
    "    kserve_op = load_component_from_file(f'{SERVE_ENPOINT_PATH}/kserve_component.yaml')\n",
    "    \n",
    "    model_name = KBM_MODEL_SERV_NAME\n",
    "    namespace = KBM_NAMESPACE\n",
    "    model_uri = f\"pvc://{PVC_NAME}/torch_model\"\n",
    "    framework=\"pytorch\"\n",
    "    \n",
    "    opt = kserve_op(action=\"apply\",\n",
    "              model_name=model_name,\n",
    "              model_uri=model_uri,\n",
    "              namespace=namespace,\n",
    "              framework=framework)\n",
    "    \n",
    "    opt.set_cpu_limit(cpu=\"2\").set_memory_limit(memory=\"4G\")\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45f4bc-c22a-4e60-b580-2ed86f34fd1d",
   "metadata": {},
   "source": [
    "## 파이프라인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f7febc1-7fe5-4472-b876-e1a36cf80622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Yelp Review NLP Model Pipeline\"\n",
    ")\n",
    "def yelp_review_nlp_model_Pipeline(\n",
    "    kc_kbm_os_train_url: str = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/data/train.csv',\n",
    "    kc_kbm_os_valid_url: str = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/data/validation.csv',\n",
    "    kc_kbm_os_handler_url: str = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/handler.py',\n",
    "    kc_kbm_os_kserve_url: str = 'https://objectstorage.kr-central-2.kakaocloud.com/v1/252267c6b6f745eba8b850ec047b673e/kbm-files/guide_docs/hands_on/yelp_review_data_nlp/kserve_component.yaml',\n",
    "    model_name: str = \"torch-model\",\n",
    "    epoch_num: str = \"10\"\n",
    "):\n",
    "    pvc1 = kubernetes.CreatePVC(\n",
    "        pvc_name=PVC_NAME,\n",
    "        access_modes=['ReadWriteOnce'],\n",
    "        size='10Gi',\n",
    "        storage_class_name=''\n",
    "    )\n",
    "    \n",
    "    ### 데이터 로드\n",
    "    download_data = download_dataset(\n",
    "        kc_kbm_os_train_url=kc_kbm_os_train_url,\n",
    "        kc_kbm_os_valid_url=kc_kbm_os_valid_url,\n",
    "        kc_kbm_os_handler_url=kc_kbm_os_handler_url,\n",
    "        kc_kbm_os_kserve_url=kc_kbm_os_kserve_url\n",
    "    )\n",
    "    download_data.set_cpu_request(cpu=\"1\").set_memory_request(memory=\"2G\")\n",
    "    download_data.set_caching_options(enable_caching=False)\n",
    "    \n",
    "    kubernetes.mount_pvc(\n",
    "        download_data,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    \n",
    "    ### 모델 학습\n",
    "    model_train = train_nlp(\n",
    "        epoch_num=epoch_num,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    model_train.set_cpu_request(cpu=\"4\").set_memory_request(memory=\"8G\")\n",
    "    model_train.set_accelerator_type(\"nvidia.com/mig-1g.10gb\").set_accelerator_limit(1)\n",
    "    kubernetes.mount_pvc(\n",
    "        model_train,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    model_train.set_display_name(\"Finetuning Text Classification Model\")\n",
    "    model_train.set_caching_options(enable_caching=False)\n",
    "    model_train.after(download_data)\n",
    "    \n",
    "    ### Mar file 생성\n",
    "    marfile = create_marfile()\n",
    "    marfile.set_caching_options(enable_caching=False)\n",
    "    marfile.set_cpu_limit(cpu=\"1\").set_memory_limit(memory=\"2G\")\n",
    "\n",
    "    kubernetes.mount_pvc(\n",
    "        marfile,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "\n",
    "    marfile.set_display_name(\"Creating Marfile\")\n",
    "    marfile.after(model_train)\n",
    "    \n",
    "    ### 모델 서빙\n",
    "    inference_model = create_inference_model()\n",
    "    inference_model.set_cpu_limit(cpu=\"4\").set_memory_limit(memory=\"8G\")\n",
    "\n",
    "    kubernetes.mount_pvc(\n",
    "        inference_model,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    inference_model.after(marfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e991c3-aa62-458a-b1c1-70232f518bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://testkbm.dev.kakaoi.io/pipeline/?ns=kbm-g-test#/experiments/details/40aa3ac9-7e86-4f47-97bb-5c849b8ce893\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://testkbm.dev.kakaoi.io/pipeline/#/runs/details/a83cf803-c77f-402f-9a5d-e0584728f6b3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=a83cf803-c77f-402f-9a5d-e0584728f6b3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = yelp_review_nlp_model_Pipeline.name + ' test experiment'\n",
    "\n",
    "run_name = yelp_review_nlp_model_Pipeline.name + ' run'\n",
    "\n",
    "arguments = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"epoch_num\": str(EPOCH_NUM)\n",
    "}\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    yelp_review_nlp_model_Pipeline, \n",
    "    experiment_name=experiment_name, \n",
    "    run_name=run_name, \n",
    "    arguments=arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5468b",
   "metadata": {},
   "source": [
    "## 파이프라인 실행 확인\n",
    "\n",
    "위 링크 또는 Kubeflow Dashoboard > Runs에서 생성하신 Run을 확인하고 \n",
    "\n",
    "Run 실행이 완료되고 난 후, 아래 테스트 코드를 실행하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262df8a6-5cb9-4ffa-8da3-8f2d9328105f",
   "metadata": {},
   "source": [
    "## 모델 서빙 API 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d750c507-3c4d-4ad8-b505-8061e4c26103",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubernetes import client, config\n",
    "from kubernetes.config import ConfigException\n",
    "\n",
    "try:\n",
    "    # Load configuration inside the Pod\n",
    "    config.load_incluster_config()\n",
    "except ConfigException:\n",
    "    # Load configuration for testing\n",
    "    config.load_kube_config()\n",
    "    \n",
    "kube_core_client = client.CoreV1Api()\n",
    "\n",
    "all_services = kube_core_client.list_namespaced_service(\n",
    "    namespace=KBM_NAMESPACE, \n",
    "    label_selector=f\"component=predictor,serving.kserve.io/inferenceservice={KBM_MODEL_SERV_NAME},networking.internal.knative.dev/serviceType=Private\"\n",
    ")\n",
    "serv_api_ip = all_services.items[0].spec.cluster_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422bf67b-fd3c-452c-a9b4-39c8cd4d0365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "if \"KUBEFLOW_HOST\" in os.environ:\n",
    "    host = os.environ[\"KUBEFLOW_HOST\"]\n",
    "\n",
    "if \"KUBEFLOW_USERNAME\" in os.environ:\n",
    "    username = os.environ[\"KUBEFLOW_USERNAME\"]\n",
    "\n",
    "if \"KUBEFLOW_PASSWORD\" in os.environ:\n",
    "    password = os.environ[\"KUBEFLOW_PASSWORD\"]\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "_kargs = {}\n",
    "if host.startswith(\"https\"):\n",
    "    _kargs[\"verify\"] = False\n",
    "\n",
    "response = session.get(\n",
    "    host, **_kargs\n",
    ")\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "}\n",
    "\n",
    "session.post(response.url, headers=headers, data={\"login\": username, \"password\": password}, **_kargs)\n",
    "session_cookie = session.cookies.get_dict()[\"authservice_session\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dace9ed-9049-43af-ad7b-de016cb8953c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값: {'instances': [{'data': 'Hello World!'}]}\n",
      "결괏값: {\"predictions\":[4]}\n"
     ]
    }
   ],
   "source": [
    "input_text_data = \"Hello World!\"\n",
    "\n",
    "_host_arr = host.split(\"/\")\n",
    "\n",
    "data = {\n",
    "    \"instances\": [{\"data\": input_text_data}]\n",
    "}\n",
    "\n",
    "x = requests.post(\n",
    "    url=f\"{host}/v1/models/{MODEL_NAME}:predict\", \n",
    "    cookies={'authservice_session': session_cookie},\n",
    "    headers={\n",
    "        \"Host\": f\"{KBM_MODEL_SERV_NAME}.{KBM_NAMESPACE}.{_host_arr[2]}\"\n",
    ",\n",
    "    },\n",
    "    json=data, **_kargs\n",
    ")\n",
    "\n",
    "print(f\"입력값: {data}\")\n",
    "print(f\"결괏값: {x.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea9e65-6b6d-4a5b-847c-fbc1c75b73de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
